{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://coinmarketcap.com/'>Coinmarketcap</a><br>\n",
    "\n",
    "### Notice: This file has been written to downloader.py, this is now used mostly for testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Market Cap Data\n",
    "In this section we gather the total market cap data from coinmarketcap. Since coinmarketcap doesn't provide any api or historical datatable for this we'll have to scrape the page for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re, sys, os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# csv filename\n",
    "file = 'data/total_market_cap.csv'\n",
    "\n",
    "## Read existing data, create new if none found\n",
    "try:\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col='Date')\n",
    "    latest_date = df.index[-1]\n",
    "    \n",
    "    # Check if latest registered data is up to date\n",
    "    if latest_date + pd.offsets.Week() >= pd.to_datetime('today'):\n",
    "        sys.exit('Data is already up to date!') # Interrupt program\n",
    "    else:\n",
    "        print('Latest data point at: ' + latest_date.strftime('%d-%m-%Y'))    \n",
    "except FileNotFoundError:\n",
    "    print('File Not Found!\\nWriting to ' + file + '...')\n",
    "    # Create empty dataframe\n",
    "    df = pd.DataFrame([], columns=['Date', 'Total Market Cap'])\n",
    "    df = df.set_index('Date')\n",
    "    # Set first data point at 20130421\n",
    "    latest_date = pd.to_datetime('20130421')  \n",
    "    \n",
    "# Create date range for historical snapshots from latest date to today-1 day since data uploads after day\n",
    "Date = pd.date_range(start=latest_date+pd.offsets.Week(), \n",
    "                     end=pd.to_datetime('today')-pd.offsets.Day(), freq='7D').strftime('%Y%m%d')\n",
    "\n",
    "market_cap = [None]*len(Date)\n",
    "# Request and return market cap value for given date from web\n",
    "def get_market_cap(date):\n",
    "    # Retrieve historical snapshot data from date\n",
    "    page = requests.get(base_url + date)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    container = body.find('div', {'class':'container'}, recursive=False)\n",
    "    mcap = container.find('span', {'id' : 'total-marketcap'}).text.strip()\n",
    "    \n",
    "    # Extract marketcap value from span\n",
    "    return int(re.sub(r',|\\$', '', mcap))\n",
    "\n",
    "## Retrieve market cap value in dollars\n",
    "base_url = 'https://coinmarketcap.com/historical/'\n",
    "print('Parsing data from {} to {}'.format(Date[0], Date[-1]))\n",
    "print('-'*40)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor: # 2 Threads seems to be optimal in jupyter\n",
    "    futures = [executor.submit(get_market_cap, date) for date in Date]\n",
    "    market_cap = [future.result() for future in futures]\n",
    "        \n",
    "## Create data frame of date\n",
    "market_cap_df = pd.DataFrame({'Date':Date, 'Total Market Cap':market_cap})\n",
    "market_cap_df.Date = pd.to_datetime(market_cap_df.Date)\n",
    "market_cap_df = market_cap_df.set_index('Date')\n",
    "\n",
    "## Write to file\n",
    "df.append(market_cap_df).to_csv(file)\n",
    "print('\\nTotal Market Cap data has been successfully updated to ' + \n",
    "      market_cap_df.index[-1].strftime('%d-%m-%Y') + '!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write/Update data for coins\n",
    "Coinmarketcap doesn't have an API to retrieve historical data, so we are going to do it by ourselves. Using bs4 we are limited to data on the specific page, therefore only data upto one month old is parsed. You can manually download the all-time data via the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path constants\n",
    "folder = 'price/'\n",
    "tail = '.csv'\n",
    "\n",
    "# Check for files in given folder, creates folder if it doesn't exist\n",
    "try:\n",
    "    filenames = os.listdir(folder)\n",
    "    folder_not_found = False\n",
    "except:\n",
    "    print('Folder not found, creating \\'' + folder + '\\'')\n",
    "    os.makedirs(folder)\n",
    "    folder_not_found = True\n",
    "\n",
    "# Get coin name and ticker from files/web if file not found\n",
    "if folder_not_found or len(filenames) < 1:\n",
    "    coin_dict = get_top_coins()      # Scrap top 9 coins from coinmarketcap\n",
    "    coins = list(coin_dict.keys())\n",
    "    coin_names = list(coin_dict.values())\n",
    "else:\n",
    "    coins = list(map(lambda x: re.sub(tail, '', x).upper(), filenames)) # Get existing coin names\n",
    "    \n",
    "    # Manual method\n",
    "    coin_names = ['cardano', 'bitcoin-cash', 'bitcoin', 'dash', 'ethereum', 'iota', 'litecoin', 'nem', 'monero', 'ripple']\n",
    "    coin_dict = dict(zip(coins, coin_names))\n",
    "    \n",
    "    # Programmer method\n",
    "    #coin_dict = get_top_coins(15)\n",
    "    #coin_names = [coin_dict[coin] for coin in coins] \n",
    "    \n",
    "# Data constants\n",
    "header = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap']\n",
    "base_url = 'https://coinmarketcap.com/currencies/'\n",
    "tail_url = '/historical-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top @nmbr_of_coins coins on coinmarketcap\n",
    "def get_top_coins(nmbr_of_coins = 9):\n",
    "    # Get main ranking table\n",
    "    page = requests.get('https://coinmarketcap.com/')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    container = body.find('div', {'class':'container'}, recursive=False)\n",
    "    table = container.find('table', {'id':'currencies'})\n",
    "\n",
    "    # Table body\n",
    "    tbody = table.find('tbody')\n",
    "    rows = tbody.find_all('tr')[:nmbr_of_coins]\n",
    "    \n",
    "    # Get the coin name and ticker from each row in table\n",
    "    coins = []\n",
    "    coin_names = []\n",
    "    for row in rows:\n",
    "        a = row.find('span', {'class':'currency-symbol'}).find('a')\n",
    "        coins.append(a.get_text())                  # Get coin ticker\n",
    "        coin_names.append(a['href'].split('/')[-2]) # Get coin name\n",
    "        \n",
    "    # Return dictionary sorted by coin ticker\n",
    "    return dict([(coin, coin_name) for coin, coin_name in sorted(zip(coins, coin_names))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = get_top_coins(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in a.items(:\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve coin historical data from coinmarketcap\n",
    "def download_coin_data(coin):\n",
    "    # Load stored data, if none found create new\n",
    "    file = folder + coin.lower() + tail\n",
    "    try:\n",
    "        original_df = pd.read_csv(file, delimiter='\\t', index_col='Date', parse_dates=True, \n",
    "                              dtype={'Open':str, 'High':str, 'Low':str, 'Close':str})\n",
    "        file_not_found = False\n",
    "        latest_date = original_df.index[0]\n",
    "        \n",
    "        # Check if data is up to date\n",
    "        if latest_date + pd.offsets.Day() >= pd.to_datetime('today'):\n",
    "            print(coin + ' data already up to date!')\n",
    "            return\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        file_not_found = True\n",
    "    \n",
    "    # Get html data\n",
    "    url = base_url + coin_dict[coin] + tail_url\n",
    "    if not file_not_found: # Only request data from date before last date\n",
    "        url += (r'?start=' + (latest_date + pd.offsets.Day()).strftime('%Y%m%d') + \n",
    "                r'&end=' + pd.to_datetime('today').strftime('%Y%m%d'))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract table data from html\n",
    "    table = soup.find('div', {'class':'table-responsive'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [e.text.strip() for e in cols]\n",
    "        data.append(cols)\n",
    "    \n",
    "    # Convert parsed data into data frame\n",
    "    parsed_df = pd.DataFrame(data, columns=header)\n",
    "    parsed_df.Date = pd.to_datetime(parsed_df.Date)\n",
    "    parsed_df = parsed_df.set_index('Date')\n",
    "    \n",
    "    # If no original file\n",
    "    if file_not_found:\n",
    "        parsed_df.to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' to ' + parsed_df.index[-1].strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully written to ' + file)\n",
    "    # Concat new and original dataframe and write to file\n",
    "    else:\n",
    "        pd.concat((parsed_df, original_df)).to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + latest_date.strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully updated to ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' and written to ' + file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download all coin data in coins concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for coin in coins:\n",
    "        executor.submit(download_coin_data, coin)\n",
    "\n",
    "print('All downloads finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://blockchain.info/'>Blockchain.info</a><br>\n",
    "\n",
    "The site provides a download url for all types of data in the same csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import downloader"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

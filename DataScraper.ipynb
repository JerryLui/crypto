{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://coinmarketcap.com/'>Coinmarketcap</a><br>\n",
    "\n",
    "#### Total Market Cap Data\n",
    "In this section we gather the total market cap data from coinmarketcap. Since coinmarketcap doesn't provide any api or historical datatable for this we'll have to scrape the page for data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Data is already up to date!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Data is already up to date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\IntelPython3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# csv filename\n",
    "file = 'data/total_market_cap.csv'\n",
    "\n",
    "## Read existing data, create new if none found\n",
    "try:\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col='Date')\n",
    "    latest_date = df.index[-1]\n",
    "    \n",
    "    # Check if latest registered data is up to date\n",
    "    if latest_date + pd.offsets.Week() >= pd.to_datetime('today'):\n",
    "        sys.exit('Data is already up to date!') # Interrupt program\n",
    "    else:\n",
    "        print('Latest data point at: ' + latest_date.strftime('%d-%m-%Y'))    \n",
    "except FileNotFoundError:\n",
    "    print('File Not Found!\\nWriting to ' + file + '...')\n",
    "    df = pd.DataFrame([], columns=['Date', 'Total Market Cap'])\n",
    "    df = df.set_index('Date')\n",
    "    latest_date = pd.to_datetime('20130421')  # Sets first data point at 20130421\n",
    "    \n",
    "# Create date range for historical snapshots from latest date to today-1 day since data uploads after day\n",
    "Date = pd.date_range(start=latest_date+pd.offsets.Week(), \n",
    "                     end=pd.to_datetime('today')-pd.offsets.Day(), freq='7D').strftime('%Y%m%d')\n",
    "\n",
    "market_cap = [None]*len(Date)\n",
    "# Request and return market cap value for given date from web\n",
    "def get_market_cap(date):\n",
    "    # Retrieve historical snapshot data from date\n",
    "    page = requests.get(base_url + date)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    container = body.find('div', {'class':'container'}, recursive=False)\n",
    "    mcap = container.find('span', {'id' : 'total-marketcap'}).text.strip()\n",
    "    \n",
    "    # Extract marketcap value from span\n",
    "    return int(re.sub(r',|\\$', '', mcap))\n",
    "\n",
    "## Retrieve market cap value in dollars\n",
    "base_url = 'https://coinmarketcap.com/historical/'\n",
    "print('Parsing data from {} to {}'.format(Date[0], Date[-1]))\n",
    "print('-'*40)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor: # 2 Threads seems to be optimal in jupyter\n",
    "    futures = [executor.submit(get_market_cap, date) for date in Date]\n",
    "    market_cap = [future.result() for future in futures]\n",
    "        \n",
    "## Create data frame of date\n",
    "market_cap_df = pd.DataFrame({'Date':Date, 'Total Market Cap':market_cap})\n",
    "market_cap_df.Date = pd.to_datetime(market_cap_df.Date)\n",
    "market_cap_df = market_cap_df.set_index('Date')\n",
    "\n",
    "## Write to file\n",
    "df.append(market_cap_df).to_csv(file)\n",
    "print('\\nTotal Market Cap data has been successfully updated to ' + \n",
    "      market_cap_df.index[-1].strftime('%d-%m-%Y') + '!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write/Update data for coins\n",
    "Coinmarketcap doesn't have an API to retrieve historical data, so we are going to do it by ourselves. Using bs4 we are limited to data on the specific page, therefore only data upto one month old is parsed. You can manually download the all-time data via the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'price/'\n",
    "tail = '.csv'\n",
    "filenames = os.listdir(folder) ## FIX WHEN NO FILES, USE TITLES INSTEAD! ##\n",
    "    \n",
    "# Get coin name of files\n",
    "coins = list(map(lambda x: re.sub(tail, '', x).upper(), filenames))\n",
    "coin_name = ['cardano', 'bitcoin-cash', 'bitcoin', 'dash', 'ethereum', 'iota', 'litecoin', 'nem', 'monero', 'ripple']\n",
    "coin_dict = dict(zip(coins, coin_name))\n",
    "\n",
    "# Data constants\n",
    "header = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap']\n",
    "base_url = 'https://coinmarketcap.com/currencies/'\n",
    "tail_url = '/historical-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA data already up to date!\n",
      "BCH data already up to date!\n",
      "IOTA data already up to date!\n",
      "BTC data already up to date!LTC data already up to date!\n",
      "XEM data already up to date!\n",
      "ETH data already up to date!\n",
      "\n",
      "DASH data already up to date!XMR data already up to date!\n",
      "\n",
      "XRP data already up to date!\n"
     ]
    }
   ],
   "source": [
    "# Retrieve coin historical data from coinmarketcap\n",
    "def download_coin_data(coin):\n",
    "    # Load stored data, if none found create new\n",
    "    file = folder + coin.lower() + tail\n",
    "    try:\n",
    "        original_df = pd.read_csv(file, delimiter='\\t', index_col='Date', parse_dates=True, \n",
    "                              dtype={'Open':str, 'High':str, 'Low':str, 'Close':str})\n",
    "        file_not_found = False\n",
    "        latest_date = original_df.index[0]\n",
    "        \n",
    "        # Check if data is up to date\n",
    "        if latest_date + pd.offsets.Day() >= pd.to_datetime('today'):\n",
    "            print(coin + ' data already up to date!')\n",
    "            return\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        file_not_found = True\n",
    "    \n",
    "    # Get html data\n",
    "    url = base_url + coin_dict[coin] + tail_url\n",
    "    if not file_not_found: # Only request data from date before last date\n",
    "        url += (r'?start=' + (latest_date + pd.offsets.Day()).strftime('%Y%m%d') + \n",
    "                r'&end=' + pd.to_datetime('today').strftime('%Y%m%d'))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract table data from html\n",
    "    table = soup.find('div', {'class':'table-responsive'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [e.text.strip() for e in cols]\n",
    "        data.append(cols)\n",
    "    \n",
    "    # Convert parsed data into data frame\n",
    "    parsed_df = pd.DataFrame(data, columns=header)\n",
    "    parsed_df.Date = pd.to_datetime(parsed_df.Date)\n",
    "    parsed_df = parsed_df.set_index('Date')\n",
    "    \n",
    "    # If no original file\n",
    "    if file_not_found:\n",
    "        parsed_df.to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' to ' + parsed_df.index[-1].strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully written to ' + file)\n",
    "    # Concat new and original dataframe and write to file\n",
    "    else:\n",
    "        pd.concat((parsed_df, original_df)).to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + latest_date.strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully updated to ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' and written to ' + file)\n",
    "        \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for coin in coins:\n",
    "        executor.submit(download_coin_data, coin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://blockchain.info/'>Blockchain.info</a><br>\n",
    "\n",
    "The site provides a download url for all types of data in the same csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File updater function for data from blockchain.info\n",
    "def blockchain_file_update(filename, url, folder='data/'):\n",
    "    # Load current existing data\n",
    "    df = pd.read_csv(folder + filename, names=['Date', 'Data'], index_col='Date', parse_dates=True)\n",
    "    url_df = pd.read_csv(url, names=['Date', 'Data'], index_col='Date', parse_dates=True)\n",
    "\n",
    "    # Replace data file if newer data available\n",
    "    if df.index[-1] < url_df.index[-1]:\n",
    "        # Check if current file data matches url data\n",
    "        if not df.isin(url_df[:df.index[-1]]).all().values:\n",
    "            sys.exit(filename + ' doesn\\'t match url data.')\n",
    "        \n",
    "        # Download url from blockchain.info\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(folder + filename, 'wb').write(r.content)  # Write to wallet file\n",
    "        print(filename + ' has been successfully updated!')\n",
    "    else:\n",
    "        print(filename + ' already up to date!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallet_users.csv has been successfully updated!\n",
      "hash_rate_raw.csv has been successfully updated!\n"
     ]
    }
   ],
   "source": [
    "filename = ['wallet_users.csv',\n",
    "            'hash_rate_raw.csv']\n",
    "url = ['https://blockchain.info/charts/my-wallet-n-users?timespan=all&format=csv',\n",
    "       'https://api.blockchain.info/charts/hash-rate?timespan=all&format=csv']\n",
    "\n",
    "for f, u in zip(filename, url):\n",
    "    blockchain_file_update(f, u)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

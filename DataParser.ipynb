{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://coinmarketcap.com/'>Coinmarketcap</a><br>\n",
    "\n",
    "#### Total Market Cap Data\n",
    "In this section we gather the total market cap data from coinmarketcap. Since coinmarketcap doesn't provide any api or historical datatable for this we'll have to crawl the page data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already up to date!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-146805f97492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlatest_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'today'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data is already up to date!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Interrupt program\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Latest data point at: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlatest_date\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d-%B-%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# csv filename\n",
    "file = 'data/total_market_cap.csv'\n",
    "\n",
    "# Read existing data, create new if none found\n",
    "try:\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col='Date')\n",
    "    latest_date = df.index[-1]\n",
    "    \n",
    "    # Check if latest registered data is up to date\n",
    "    if latest_date + pd.offsets.Week() > pd.to_datetime('today'):\n",
    "        print('Data is already up to date!')\n",
    "        assert(False) # Interrupt program\n",
    "    else:\n",
    "        print('Latest data point at: ' + latest_date.strftime('%d-%B-%Y'))    \n",
    "except FileNotFoundError:\n",
    "    print('File Not Found!\\nWriting to ' + file + '...')\n",
    "    df = pd.DataFrame([], columns=['Date', 'Total Market Cap'])\n",
    "    df = df.set_index('Date')\n",
    "    latest_date = pd.to_datetime('20130421')  # Sets first data point at 20130421\n",
    "    \n",
    "# Create date range for historical snapshots from latest date to today\n",
    "Date = pd.date_range(start=latest_date+pd.offsets.Week(), end='today', freq='7D').strftime('%Y%m%d')\n",
    "\n",
    "# Retrieve market cap value in dollars\n",
    "market_cap = []\n",
    "base_url = 'https://coinmarketcap.com/historical/'\n",
    "for i, date in enumerate(Date):\n",
    "    # Retrieve historical snapshot data from date\n",
    "    page = requests.get(base_url + date)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract marketcap value from span\n",
    "    market_cap.append(int(re.sub(r',|\\$', '', soup.find('span', {'id' : 'total-marketcap'}).text.strip())))\n",
    "    \n",
    "    ### TODO: Implement progressbar\n",
    "    \n",
    "# Create data frame of data\n",
    "market_cap_df = pd.DataFrame({'Date':Date, 'Total Market Cap':market_cap})\n",
    "market_cap_df.Date = pd.to_datetime(market_cap_df.Date)\n",
    "market_cap_df = market_cap_df.set_index('Date')\n",
    "\n",
    "# Write to file\n",
    "df.append(market_cap_df).to_csv(file)\n",
    "print('\\nTotal Market Cap data has been successfully updated to!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write/Update data for coins\n",
    "Coinmarketcap doesn't have an API to retrieve historical data, so we are going to do it by ourselves. Using bs4 we are limited to data on the specific page, therefore only data upto one month old is parsed. You can manually download the all-time data via the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'price/'\n",
    "tail = '.csv'\n",
    "filenames = os.listdir(folder)\n",
    "    \n",
    "# Get coin name of files\n",
    "coins = list(map(lambda x: re.sub(tail, '', x).upper(), filenames))\n",
    "coin_name = ['cardano', 'bitcoin-cash', 'bitcoin', 'dash', 'ethereum', 'iota', 'litecoin', 'nem', 'monero', 'ripple']\n",
    "coin_dict = dict(zip(coins, coin_name))\n",
    "\n",
    "# Data constants\n",
    "header = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap']\n",
    "base_url = 'https://coinmarketcap.com/currencies/'\n",
    "tail_url = '/historical-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Parsing ADA data...\n",
      "ADA data already up to date!\n",
      "----------------------------------------\n",
      "Parsing BCH data...\n",
      "BCH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing BTC data...\n",
      "BTC data already up to date!\n",
      "----------------------------------------\n",
      "Parsing DASH data...\n",
      "DASH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing ETH data...\n",
      "ETH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing IOTA data...\n",
      "IOTA data already up to date!\n",
      "----------------------------------------\n",
      "Parsing LTC data...\n",
      "LTC data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XEM data...\n",
      "XEM data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XMR data...\n",
      "XMR data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XRP data...\n",
      "XRP data already up to date!\n"
     ]
    }
   ],
   "source": [
    "# Go through and update all coin data\n",
    "for coin in coins:\n",
    "    print('-'*40)\n",
    "    print('Parsing ' + coin + ' data...')\n",
    "    # Load stored data, if none found create new\n",
    "    file = folder + coin.lower() + tail\n",
    "    try:\n",
    "        original_df = pd.read_csv(file, delimiter='\\t', index_col='Date', parse_dates=True, \n",
    "                              dtype={'Open':str, 'High':str, 'Low':str, 'Close':str})\n",
    "        file_not_found = False\n",
    "        latest_date = original_df.index[0]\n",
    "        \n",
    "        # Check if data is up to date\n",
    "        if latest_date + pd.offsets.Day() >= pd.to_datetime('today'):\n",
    "            print(coin + ' data already up to date!')\n",
    "            continue\n",
    "        else:\n",
    "            print('Latest data point at: ' + latest_date.strftime('%Y-%m-%d'))\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print('File not found!\\nCreating ' + coin + tail)\n",
    "        file_not_found = True\n",
    "    \n",
    "    # Get html data\n",
    "    url = base_url + coin_dict[coin] + tail_url\n",
    "    if not file_not_found:\n",
    "        url += (r'?start=' + (latest_date + pd.offsets.Day()).strftime('%Y%m%d') + \n",
    "                r'&end=' + pd.to_datetime('today').strftime('%Y%m%d'))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract table data from html\n",
    "    table = soup.find('div', {'class':'table-responsive'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [e.text.strip() for e in cols]\n",
    "        data.append(cols)\n",
    "    \n",
    "    # Convert parsed data into data frame\n",
    "    parsed_df = pd.DataFrame(data, columns=header)\n",
    "    parsed_df.Date = pd.to_datetime(parsed_df.Date)\n",
    "    parsed_df = parsed_df.set_index('Date')\n",
    "\n",
    "    # Check if data from the same day are equal\n",
    "    #print(parsed_df[parsed_df.index == latest_date] == original_df.iloc[0])\n",
    "    \n",
    "    # If no original file\n",
    "    if file_not_found:\n",
    "        parsed_df.to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' to ' + parsed_df.index[-1].strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully written to ' + file)\n",
    "    # Concat new and original dataframe and write to file\n",
    "    else:\n",
    "        pd.concat((parsed_df, original_df)).to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + latest_date.strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully updated to ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' and written to ' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='https://blockchain.info/'>Blockchain.info</a><br>\n",
    "\n",
    "The site provides a download url for all types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64571"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File constants\n",
    "folder = 'data/'\n",
    "\n",
    "# Download url from blockchain.info\n",
    "url = 'https://api.blockchain.info/charts/hash-rate?timespan=all&format=csv'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(folder + 'hash_rate_raw.csv', 'wb').write(r.content)  # Write to hash rate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://blockchain.info/charts/my-wallet-n-users?timespan=all&format=csv'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(folder + 'wallet_users.csv', 'wb').write(r.content)  # Write to wallet file"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

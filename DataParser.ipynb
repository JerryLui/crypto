{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse from coinmarketcap for historical total market cap data\n",
    "Since coinmarketcap doesn't provide any api or historical datatable for this we'll have to crawl the page data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already up to date!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c21ba97f7012>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlatest_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'today'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data is already up to date!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Interrupt program\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Latest data point at: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlatest_date\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d-%B-%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# csv filename\n",
    "file = 'data/total_market_cap.csv'\n",
    "\n",
    "# Read existing data, create new if none found\n",
    "try:\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col='Date')\n",
    "    latest_date = df.index[-1]\n",
    "    \n",
    "    # Check if latest registered data is up to date\n",
    "    if latest_date + pd.offsets.Week() > pd.to_datetime('today'):\n",
    "        print('Data is already up to date!')\n",
    "        assert(False) # Interrupt program\n",
    "    else:\n",
    "        print('Latest data point at: ' + latest_date.strftime('%d-%B-%Y'))    \n",
    "except FileNotFoundError:\n",
    "    print('File Not Found!\\nWriting to ' + file + '...')\n",
    "    df = pd.DataFrame([], columns=['Date', 'Total Market Cap'])\n",
    "    df = df.set_index('Date')\n",
    "    latest_date = pd.to_datetime('20130421')  # Sets first data point at 20130421\n",
    "    \n",
    "# Create date range for historical snapshots from latest date to today\n",
    "Date = pd.date_range(start=latest_date+pd.offsets.Week(), end='today', freq='7D').strftime('%Y%m%d')\n",
    "\n",
    "# Retrieve market cap value in dollars\n",
    "market_cap = []\n",
    "base_url = 'https://coinmarketcap.com/historical/'\n",
    "for i, date in enumerate(Date):\n",
    "    # Retrieve historical snapshot data from date\n",
    "    page = requests.get(base_url + date)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract marketcap value from span\n",
    "    market_cap.append(int(re.sub(r',|\\$', '', soup.find('span', {'id' : 'total-marketcap'}).text.strip())))\n",
    "    \n",
    "    ### TODO: Implement progressbar\n",
    "    \n",
    "# Create data frame of data\n",
    "market_cap_df = pd.DataFrame({'Date':Date, 'Total Market Cap':market_cap})\n",
    "market_cap_df.Date = pd.to_datetime(market_cap_df.Date)\n",
    "market_cap_df = market_cap_df.set_index('Date')\n",
    "\n",
    "# Write to file\n",
    "df.append(market_cap_df).to_csv(file)\n",
    "print('\\nTotal Market Cap data has been successfully updated!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write/Update data for coins\n",
    "Coinmarketcap doesn't have an API to retrieve historical data, so we are going to do it by ourselves. Using bs4 we are limited to data on the specific page, therefore only data upto one month old is parsed. You can manually download the all-time data via the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'price/'\n",
    "tail = '.csv'\n",
    "filenames = os.listdir(folder)\n",
    "    \n",
    "# Get coin name of files\n",
    "coins = list(map(lambda x: re.sub(tail, '', x).upper(), filenames))\n",
    "coin_name = ['bitcoin-cash', 'bitcoin', 'dash', 'ethereum', 'iota', 'litecoin', 'nem', 'monero', 'ripple']\n",
    "coin_dict = dict(zip(coins, coin_name))\n",
    "\n",
    "# Data constants\n",
    "header = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap']\n",
    "base_url = 'https://coinmarketcap.com/currencies/'\n",
    "tail_url = '/historical-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Parsing BCH data...\n",
      "Latest data point at: 2017-12-12\n",
      "BCH data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/bch.csv\n",
      "----------------------------------------\n",
      "Parsing BTC data...\n",
      "Latest data point at: 2017-12-12\n",
      "BTC data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/btc.csv\n",
      "----------------------------------------\n",
      "Parsing DASH data...\n",
      "Latest data point at: 2017-12-12\n",
      "DASH data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/dash.csv\n",
      "----------------------------------------\n",
      "Parsing ETH data...\n",
      "Latest data point at: 2017-12-12\n",
      "ETH data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/eth.csv\n",
      "----------------------------------------\n",
      "Parsing IOTA data...\n",
      "Latest data point at: 2017-12-12\n",
      "IOTA data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/iota.csv\n",
      "----------------------------------------\n",
      "Parsing LTC data...\n",
      "Latest data point at: 2017-12-12\n",
      "LTC data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/ltc.csv\n",
      "----------------------------------------\n",
      "Parsing XEM data...\n",
      "Latest data point at: 2017-12-12\n",
      "XEM data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/xem.csv\n",
      "----------------------------------------\n",
      "Parsing XMR data...\n",
      "Latest data point at: 2017-12-12\n",
      "XMR data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/xmr.csv\n",
      "----------------------------------------\n",
      "Parsing XRP data...\n",
      "Latest data point at: 2017-12-12\n",
      "XRP data from 12-December-2017 has successfully  been updated to 13-December-2017 and written to price/xrp.csv\n"
     ]
    }
   ],
   "source": [
    "# Go through and update all coin data\n",
    "for coin in coins:\n",
    "    print('-'*40)\n",
    "    print('Parsing ' + coin + ' data...')\n",
    "    # Load stored data, if none found create new\n",
    "    file = folder + coin.lower() + tail\n",
    "    try:\n",
    "        original_df = pd.read_csv(file, delimiter='\\t', index_col='Date', parse_dates=True, \n",
    "                              dtype={'Open':str, 'High':str, 'Low':str, 'Close':str})\n",
    "        file_not_found = False\n",
    "        latest_date = original_df.index[0]\n",
    "        \n",
    "        # Check if data is up to date\n",
    "        if latest_date + pd.offsets.Day() >= pd.to_datetime('today'):\n",
    "            print(coin + ' data already up to date!')\n",
    "            continue\n",
    "        else:\n",
    "            print('Latest data point at: ' + latest_date.strftime('%Y-%m-%d'))\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print('File not found!\\nCreating ' + coin + tail)\n",
    "        file_not_found = True\n",
    "    \n",
    "    # Get html data\n",
    "    url = base_url + coin_dict[coin] + tail_url\n",
    "    if not file_not_found:\n",
    "        url += (r'?start=' + (latest_date + pd.offsets.Day()).strftime('%Y%m%d') + \n",
    "                r'&end=' + pd.to_datetime('today').strftime('%Y%m%d'))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract table data from html\n",
    "    table = soup.find('div', {'class':'table-responsive'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [e.text.strip() for e in cols]\n",
    "        data.append(cols)\n",
    "    \n",
    "    # Convert parsed data into data frame\n",
    "    parsed_df = pd.DataFrame(data, columns=header)\n",
    "    parsed_df.Date = pd.to_datetime(parsed_df.Date)\n",
    "    parsed_df = parsed_df.set_index('Date')\n",
    "\n",
    "    # Check if data from the same day are equal\n",
    "    #print(parsed_df[parsed_df.index == latest_date] == original_df.iloc[0])\n",
    "    \n",
    "    # If no original file\n",
    "    if file_not_found:\n",
    "        parsed_df.to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' to ' + parsed_df.index[-1].strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully written to ' + file)\n",
    "    # Concat new and original dataframe and write to file\n",
    "    else:\n",
    "        pd.concat((parsed_df, original_df)).to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + latest_date.strftime('%d-%B-%Y') + \n",
    "                      ' has successfully  been updated to ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' and written to ' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

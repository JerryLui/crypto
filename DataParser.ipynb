{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse from coinmarketcap for historical total market cap data\n",
    "Since coinmarketcap doesn't provide any api or historical datatable for this we'll have to crawl the page data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already up to date!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\IntelPython3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# csv filename\n",
    "file = 'data/total_market_cap.csv'\n",
    "\n",
    "# Read existing data, create new if none found\n",
    "try:\n",
    "    df = pd.read_csv(file, parse_dates=True, index_col='Date')\n",
    "    latest_date = df.index[-1]\n",
    "    \n",
    "    # Check if latest registered data is up to date\n",
    "    if latest_date + pd.offsets.Week() > pd.to_datetime('today'):\n",
    "        print('Data is already up to date!')\n",
    "        import sys\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print('Latest data point at: ' + latest_date.strftime('%d-%B-%Y'))    \n",
    "except FileNotFoundError:\n",
    "    print('File Not Found!\\nWriting to ' + file)\n",
    "    df = pd.DataFrame([], columns=['Date', 'Total Market Cap'])\n",
    "    df = df.set_index('Date')\n",
    "    latest_date = pd.to_datetime('20130428')\n",
    "    \n",
    "# Create date range for historical snapshots from latest date to today\n",
    "Date = pd.date_range(start=latest_date+pd.offsets.Week(), end='today', freq='7D').strftime('%Y%m%d')\n",
    "\n",
    "# Retrieve market cap value in dollars\n",
    "market_cap = []\n",
    "base_url = 'https://coinmarketcap.com/historical/'\n",
    "for i, date in enumerate(Date):\n",
    "    # Retrieve historical snapshot data from date\n",
    "    page = requests.get(base_url + date)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract marketcap value from span\n",
    "    market_cap.append(int(re.sub(r',|\\$', '', soup.find('span', {'id' : 'total-marketcap'}).text.strip())))\n",
    "\n",
    "# Create data frame of data\n",
    "market_cap_df = pd.DataFrame({'Date':Date, 'Total Market Cap':market_cap})\n",
    "market_cap_df.Date = pd.to_datetime(market_cap_df.Date)\n",
    "market_cap_df = market_cap_df.set_index('Date')\n",
    "\n",
    "# Write to file\n",
    "df.append(market_cap_df).to_csv(file)\n",
    "print('\\nTotal Market Cap data has been successfully updated!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write/Update data for coins\n",
    "Coinmarketcap doesn't have an API to retrieve historical data, so we are going to do it by ourselves. Using bs4 we are limited to data on the specific page, therefore only data upto one month old is parsed. You can manually download the all-time data via the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = 'price/'\n",
    "tail = '.csv'\n",
    "filenames = os.listdir(folder)\n",
    "    \n",
    "# Get coin name of files\n",
    "coins = list(map(lambda x: re.sub(tail, '', x).upper(), filenames))\n",
    "coin_name = ['bitcoin-cash', 'bitcoin', 'dash', 'ethereum', 'iota', 'litecoin', 'nem', 'monero', 'ripple']\n",
    "coin_dict = dict(zip(coins, coin_name))\n",
    "\n",
    "# Data constants\n",
    "header = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap']\n",
    "base_url = 'https://coinmarketcap.com/currencies/'\n",
    "tail_url = '/historical-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Parsing BCH data...\n",
      "BCH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing BTC data...\n",
      "BTC data already up to date!\n",
      "----------------------------------------\n",
      "Parsing DASH data...\n",
      "DASH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing ETH data...\n",
      "ETH data already up to date!\n",
      "----------------------------------------\n",
      "Parsing IOTA data...\n",
      "IOTA data already up to date!\n",
      "----------------------------------------\n",
      "Parsing LTC data...\n",
      "LTC data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XEM data...\n",
      "XEM data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XMR data...\n",
      "XMR data already up to date!\n",
      "----------------------------------------\n",
      "Parsing XRP data...\n",
      "XRP data already up to date!\n"
     ]
    }
   ],
   "source": [
    "# Go through and update all coin data\n",
    "for coin in coins:\n",
    "    print('-'*40)\n",
    "    print('Parsing ' + coin + ' data...')\n",
    "    # Load stored data, if none found create new\n",
    "    file = folder + coin.lower() + tail\n",
    "    try:\n",
    "        original_df = pd.read_csv(file, delimiter='\\t', index_col='Date', parse_dates=True, \n",
    "                              dtype={'Open':str, 'High':str, 'Low':str, 'Close':str})\n",
    "        file_not_found = False\n",
    "        latest_date = original_df.index[0]\n",
    "        \n",
    "        # Check if data is up to date\n",
    "        if latest_date + pd.offsets.Day() >= pd.to_datetime('today'):\n",
    "            print(coin + ' data already up to date!')\n",
    "            continue\n",
    "        else:\n",
    "            print('Latest data point at: ' + latest_date.strftime('%Y-%m-%d'))\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print('File not found!\\nCreating ' + coin + tail)\n",
    "        file_not_found = True\n",
    "    \n",
    "    # Get html data\n",
    "    url = base_url + coin_dict[coin] + tail_url\n",
    "    if not file_not_found:\n",
    "        url += (r'?start=' + (latest_date + pd.offsets.Day()).strftime('%Y%m%d') + \n",
    "                r'&end=' + pd.to_datetime('today').strftime('%Y%m%d'))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract table data from html\n",
    "    table = soup.find('div', {'class':'table-responsive'})\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [e.text.strip() for e in cols]\n",
    "        data.append(cols)\n",
    "    \n",
    "    # Convert parsed data into data frame\n",
    "    parsed_df = pd.DataFrame(data, columns=header)\n",
    "    parsed_df.Date = pd.to_datetime(parsed_df.Date)\n",
    "    parsed_df = parsed_df.set_index('Date')\n",
    "\n",
    "    # Check if data from the same day are equal\n",
    "    #print(parsed_df[parsed_df.index == latest_date] == original_df.iloc[0])\n",
    "    \n",
    "    # If no original file\n",
    "    if file_not_found:\n",
    "        parsed_df.to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' to ' + parsed_df.index[-1].strftime('%d-%B-%Y') + \n",
    "                      ' has been successfully written to ' + file)\n",
    "    # Concat new and original dataframe and write to file\n",
    "    else:\n",
    "        pd.concat((parsed_df, original_df)).to_csv(file, sep='\\t')\n",
    "        print(coin + ' data from ' + latest_date.strftime('%d-%B-%Y') + \n",
    "                      ' has successfully  been updated to ' + parsed_df.index[0].strftime('%d-%B-%Y') + \n",
    "                      ' and written to ' + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
